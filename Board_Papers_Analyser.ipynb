{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "provenance": [],
   "name": "NHS Board Papers Analyser.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# NHS Board Papers Analyser\n\nEnter a trust name, run all cells in order, and receive structured story leads.\n\n**You will need:**\n- An [Anthropic API key](https://console.anthropic.com) (create a free account → API Keys)\n- `prompt_template.txt` uploaded to the Colab files panel (folder icon, left sidebar)\n\n**Cost per run:** approximately £0.50–£1.50 using Claude Opus. Use `claude-sonnet-4-6` for ~5x cheaper.\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Suppress warnings and install dependencies\n# Run this first, once per session.\n\nimport warnings, os, sys, logging\nwarnings.filterwarnings('ignore')\nos.environ['PYTHONWARNINGS'] = 'ignore'\nlogging.captureWarnings(True)\nlogging.getLogger('py.warnings').setLevel(logging.CRITICAL)\n\ntry:\n    import jupyter_client.session as _jcs\n    import datetime as _dt\n    _jcs.utcnow = lambda: _dt.datetime.now(_dt.timezone.utc).replace(tzinfo=None)\nexcept Exception:\n    pass\n\n!pip install -q anthropic pypdfium2 requests beautifulsoup4\nprint('Ready.')\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Configure here\n\nEdit the values below and run the cell.\n\n> **If the search fails** (Cell 4 says 'Search failed'), paste the trust's board papers\n> page URL into `MANUAL_BOARD_PAPERS_URL` in Cell 2 and re-run from Cell 4.\n>\n> **Serper API key** is needed for search. Get a free key (2,500 searches/month)\n> at [serper.dev](https://serper.dev).\n>\n> **If the download fails**, download the PDF manually, upload via the files panel,\n> set `MANUAL_PDF_PATH` to the filename, and re-run from Cell 5.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Configuration -- edit these values\n\nANTHROPIC_API_KEY = 'sk-ant-...'   # Your Anthropic API key\nSERPER_API_KEY    = ''              # Your Serper.dev key (free at serper.dev)\nTRUST_NAME        = 'NHS Gloucestershire ICB'\nMODEL             = 'claude-opus-4-6'   # or 'claude-sonnet-4-6' for cheaper runs\n\n# If Cell 4 search fails: paste the board papers page URL here and re-run\nMANUAL_BOARD_PAPERS_URL = ''\n\n# If Cell 5 download fails: upload PDF manually, set filename here, re-run from Cell 5\nMANUAL_PDF_PATH = ''\n\n# -- Validation --\nif not ANTHROPIC_API_KEY.startswith('sk-'):\n    print('WARNING: Anthropic API key looks wrong -- should start with sk-ant-')\nif not SERPER_API_KEY:\n    print('NOTE: No Serper key set. Search will only use the built-in database.')\nprint(f'Trust:   {TRUST_NAME}')\nprint(f'Model:   {MODEL}')\nprint(f'API key: set ({ANTHROPIC_API_KEY[:12]}...)')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Imports and helper functions\n\nimport os, io, re, zipfile, tempfile, warnings, logging, urllib.parse, json as _json\nfrom pathlib import Path\nfrom urllib.parse import urljoin\nwarnings.filterwarnings('ignore')\nlogging.captureWarnings(True)\nlogging.getLogger('py.warnings').setLevel(logging.CRITICAL)\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pypdfium2 as pdfium\nimport anthropic\n\nFALLBACK_UAS = [\n    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n    '(KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',\n    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 '\n    '(KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',\n]\n\nCHARS_PER_PAGE = 3000\nCHAR_LIMIT     = 400_000\n\n# ---------------------------------------------------------------------------\n# Trust URL database: lookup table + Serper fallback\n# ---------------------------------------------------------------------------\n\n_TRUST_DB = None\n\ndef _load_trust_db():\n    global _TRUST_DB\n    if _TRUST_DB is not None:\n        return _TRUST_DB\n    # Try to fetch latest from GitHub\n    raw_url = ('https://raw.githubusercontent.com/Davewest84/'\n               'nhs-board-papers-reader/main/trust_urls.json')\n    try:\n        r = requests.get(raw_url, timeout=10)\n        if r.status_code == 200:\n            _TRUST_DB = r.json()\n            print(f'  Database loaded: {len(_TRUST_DB)} organisations')\n            return _TRUST_DB\n    except Exception:\n        pass\n    # Fallback: try local copy\n    for p in [Path('/content/trust_urls.json'), Path('trust_urls.json')]:\n        if p.exists():\n            _TRUST_DB = _json.loads(p.read_text())\n            print(f'  Database loaded (local): {len(_TRUST_DB)} organisations')\n            return _TRUST_DB\n    _TRUST_DB = []\n    print('  WARNING: Trust database not available.')\n    return _TRUST_DB\n\ndef _normalize(s):\n    s = s.lower().strip()\n    s = re.sub(r'^nhs\\s+', '', s)\n    for suffix in [' nhs foundation trust', ' nhs trust', ' foundation trust',\n                   ' nhs', ' integrated care board', ' icb']:\n        if s.endswith(suffix):\n            s = s[:-len(suffix)].strip()\n    return re.sub(r'\\s+', ' ', s).strip()\n\ndef _lookup_trust(trust_name, trust_db):\n    query = _normalize(trust_name)\n    best_url, best_score = None, 0\n    for entry in trust_db:\n        for name in entry['names']:\n            stored = _normalize(name)\n            if query == stored:\n                return entry['url']   # exact match wins immediately\n            if query in stored:\n                score = len(query)\n            elif stored in query:\n                score = len(stored)\n            else:\n                continue\n            if score > best_score:\n                best_score, best_url = score, entry['url']\n    return best_url\n\ndef _verify_url(url):\n    \"\"\"Return url if it responds HTTP < 400, else None.\"\"\"\n    headers = {'User-Agent': FALLBACK_UAS[0]}\n    try:\n        r = requests.head(url, timeout=10, allow_redirects=True, headers=headers)\n        if r.status_code < 400:\n            return url\n        # Some servers reject HEAD -- try GET\n        r2 = requests.get(url, timeout=10, allow_redirects=True,\n                          headers=headers, stream=True)\n        r2.close()\n        if r2.status_code < 400:\n            return url\n    except Exception:\n        pass\n    return None\n\ndef _search_serper(trust_name, api_key):\n    \"\"\"Search via Serper.dev (Google search API -- works from cloud IPs).\"\"\"\n    endpoint = 'https://google.serper.dev/search'\n    headers = {'X-API-KEY': api_key, 'Content-Type': 'application/json'}\n    board_kws = ['board-papers', 'board-meeting', 'board-meetings', 'trust-board',\n                 'board_papers', 'board-pack', 'board-meetings-and-papers',\n                 'governance/board']\n    queries = [\n        f'\"{trust_name}\" board papers site:nhs.uk',\n        f'\"{trust_name}\" board meeting papers 2026',\n    ]\n    for q in queries:\n        try:\n            resp = requests.post(endpoint, json={'q': q, 'num': 10, 'gl': 'uk'},\n                                 headers=headers, timeout=15)\n            data = resp.json()\n            for result in data.get('organic', []):\n                link = result.get('link', '')\n                if any(kw in link.lower() for kw in board_kws):\n                    return link\n        except Exception as e:\n            print(f'    Serper error: {e}')\n            break\n    return None\n\ndef find_board_papers_url(trust_name):\n    \"\"\"Step 1: database lookup + verify. Step 2: Serper if needed.\"\"\"\n    trust_db = _load_trust_db()\n\n    if trust_db:\n        candidate = _lookup_trust(trust_name, trust_db)\n        if candidate:\n            print(f'  Database match: {candidate}')\n            if _verify_url(candidate):\n                print('  URL verified OK.')\n                return candidate\n            print('  Stored URL appears broken -- searching for updated URL...')\n\n    serper_key = SERPER_API_KEY\n    if serper_key and serper_key.strip():\n        print('  Searching via Serper...')\n        url = _search_serper(trust_name, serper_key)\n        if url:\n            print(f'  Found via Serper: {url}')\n            print('  (If this is correct and recurring, add to trust_urls.json)')\n            return url\n    else:\n        print('  No Serper key set -- add SERPER_API_KEY in Cell 2 for search fallback.')\n\n    return None\n\n# ---------------------------------------------------------------------------\n# Fetch index page and find document links\n# ---------------------------------------------------------------------------\n\ndef get_document_links(session, index_url):\n    try:\n        resp = session.get(index_url, timeout=30)\n        resp.raise_for_status()\n    except Exception as e:\n        print(f'  Could not fetch index page: {e}')\n        return []\n    soup = BeautifulSoup(resp.text, 'html.parser')\n    links, seen = [], set()\n    doc_exts = ('.pdf', '.zip', '.docx')\n    doc_kws  = ['download', 'document', '/file', 'attachment', 'board-paper']\n    for a in soup.find_all('a', href=True):\n        href = a['href']\n        text = a.get_text(strip=True) or href\n        h = href.lower()\n        if any(h.endswith(e) for e in doc_exts) or any(k in h for k in doc_kws):\n            full = href if href.startswith('http') else urljoin(index_url, href)\n            if full not in seen:\n                seen.add(full)\n                links.append({'text': text[:100], 'url': full})\n    return links\n\ndef pick_best_link(links):\n    if not links: return None\n    priority = ['2026', '2025', 'january', 'february', 'march', 'november',\n                'board-pack', 'combined', 'agenda']\n    for link in links:\n        if any(t in (link['text'] + ' ' + link['url']).lower() for t in priority):\n            return link['url']\n    for link in links:\n        if '.pdf' in link['url'].lower():\n            return link['url']\n    return links[0]['url']\n\n# ---------------------------------------------------------------------------\n# Download\n# ---------------------------------------------------------------------------\n\ndef download_file(session, url, referer):\n    for i, ua in enumerate(FALLBACK_UAS):\n        headers = {'User-Agent': ua, 'Referer': referer,\n                   'Accept': 'application/pdf,application/zip,*/*'}\n        try:\n            resp = session.get(url, headers=headers, timeout=120)\n            if resp.status_code == 200 and len(resp.content) > 10_000:\n                return resp.content\n            print(f'  Attempt {i+1}: HTTP {resp.status_code}')\n        except Exception as e:\n            print(f'  Attempt {i+1} failed: {e}')\n    return None\n\ndef save_and_unpack(data, save_dir):\n    os.makedirs(save_dir, exist_ok=True)\n    if data[:2] == b'PK':\n        print('  ZIP detected -- extracting PDFs...')\n        paths = []\n        try:\n            with zipfile.ZipFile(io.BytesIO(data)) as zf:\n                for name in zf.namelist():\n                    if name.lower().endswith('.pdf') and not name.startswith('__MACOSX'):\n                        safe = os.path.basename(name) or f'file_{len(paths)}.pdf'\n                        out = os.path.join(save_dir, safe)\n                        with open(out, 'wb') as f: f.write(zf.read(name))\n                        paths.append(out)\n                        print(f'  Extracted: {safe}')\n        except zipfile.BadZipFile:\n            print('  ZIP extraction failed.')\n        return paths\n    else:\n        out = os.path.join(save_dir, 'board_papers.pdf')\n        with open(out, 'wb') as f: f.write(data)\n        print(f'  Saved: board_papers.pdf ({len(data):,} bytes)')\n        return [out]\n\n# ---------------------------------------------------------------------------\n# Text extraction\n# ---------------------------------------------------------------------------\n\ndef extract_pages(pdf, start, end):\n    parts = []\n    for i in range(start, min(end, len(pdf))):\n        try:\n            text = pdf[i].get_textpage().get_text_range()\n            if text.strip():\n                parts.append(f'-- Page {i+1} --\\n{text[:CHARS_PER_PAGE]}')\n        except Exception:\n            pass\n    return '\\n'.join(parts)\n\ndef find_section_starts(agenda_text, total):\n    patterns = {\n        'ceo_report':  r'chief executive[^\\n]{0,60}?(\\d{1,3})\\b',\n        'finance':     r'finance report[^\\n]{0,60}?(\\d{1,3})\\b',\n        'performance': r'(?:integrated performance|ipr)[^\\n]{0,60}?(\\d{1,3})\\b',\n        'quality':     r'quality[^\\n]{0,60}?(\\d{1,3})\\b',\n        'workforce':   r'(?:people committee|workforce)[^\\n]{0,60}?(\\d{1,3})\\b',\n    }\n    secs = {}\n    for name, pat in patterns.items():\n        m = re.search(pat, agenda_text.lower())\n        if m:\n            p = int(m.group(1))\n            if 3 <= p <= total:\n                secs[name] = p - 1\n    return secs\n\ndef extract_targeted_text(pdf_paths):\n    all_secs = {}\n    for pdf_path in pdf_paths:\n        label = os.path.basename(pdf_path)\n        print(f'  Reading: {label}')\n        try:\n            pdf = pdfium.PdfDocument(pdf_path)\n        except Exception as e:\n            print(f'  Could not open: {e}')\n            continue\n        total = len(pdf)\n        print(f'  Pages: {total}')\n        agenda = extract_pages(pdf, 0, min(6, total))\n        all_secs[f'{label}__agenda'] = agenda\n        secs = find_section_starts(agenda, total)\n        if secs:\n            print(f'  Sections found: {list(secs.keys())}')\n            for sname, start in secs.items():\n                all_secs[f'{label}__{sname}'] = extract_pages(pdf, start, min(start+30, total))\n        else:\n            print('  No agenda page refs -- reading in thirds')\n            chunk = max(20, total // 3)\n            all_secs[f'{label}__part_1'] = extract_pages(pdf, 0, chunk)\n            all_secs[f'{label}__part_2'] = extract_pages(pdf, chunk, chunk*2)\n            all_secs[f'{label}__part_3'] = extract_pages(pdf, chunk*2, total)\n    return all_secs\n\n# ---------------------------------------------------------------------------\n# Prompt template\n# ---------------------------------------------------------------------------\n\n_template_paths = [Path('/content/prompt_template.txt'), Path('prompt_template.txt')]\nPROMPT_TEMPLATE = None\nfor _p in _template_paths:\n    if _p.exists():\n        PROMPT_TEMPLATE = _p.read_text(encoding='utf-8')\n        print('Prompt template loaded.')\n        break\n\nprint('Helper functions ready.')\nif not PROMPT_TEMPLATE:\n    print('\\nWARNING: prompt_template.txt not found.')\n    print('Upload it via the files panel (folder icon, left sidebar).')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Find board papers page and document links\n\npdf_paths = []\n\nif MANUAL_PDF_PATH:\n    print(f\"Using manually provided PDF: {MANUAL_PDF_PATH}\")\n    board_papers_url = MANUAL_BOARD_PAPERS_URL or \"(manual upload)\"\n    selected_url = MANUAL_PDF_PATH\n    pdf_paths = [MANUAL_PDF_PATH]\nelse:\n    board_papers_url = MANUAL_BOARD_PAPERS_URL\n\n    if not board_papers_url:\n        print(f\"Searching for: {TRUST_NAME}\")\n        board_papers_url = find_board_papers_url(TRUST_NAME)\n\n    if board_papers_url:\n        print(f\"\\nBoard papers page: {board_papers_url}\")\n    else:\n        print(\"\\n\" + \"=\"*60)\n        print(\"Search failed — all three methods tried. Action needed:\")\n        print(\"  1. Visit the trust website in your browser\")\n        print(\"  2. Find the board papers or board meetings page\")\n        print(\"  3. Copy that page URL\")\n        print(\"  4. Paste it into MANUAL_BOARD_PAPERS_URL in Cell 2\")\n        print(\"  5. Re-run Cell 2, then this cell\")\n        print(\"=\"*60)\n        raise SystemExit(\"Set MANUAL_BOARD_PAPERS_URL and re-run.\")\n\n    session = requests.Session()\n    session.headers[\"User-Agent\"] = FALLBACK_UAS[0]\n    try: session.get(board_papers_url, timeout=20)\n    except Exception: pass\n\n    print(\"\\nFetching document links...\")\n    links = get_document_links(session, board_papers_url)\n\n    if links:\n        print(f\"Found {len(links)} document link(s):\")\n        for i, link in enumerate(links[:15]):\n            print(f\"  [{i}] {link['text'][:70]}\")\n        selected_url = pick_best_link(links)\n        print(f\"\\nAuto-selected: {selected_url}\")\n        print(\"\\nTo use a different link: paste its URL into MANUAL_BOARD_PAPERS_URL\")\n        print(\"in Cell 2, then re-run Cell 2 and skip straight to Cell 5.\")\n    else:\n        print(\"\\n\" + \"=\"*60)\n        print(\"No document links found on the index page. Action needed:\")\n        print(\"  1. Visit the board papers page in your browser:\")\n        print(f\"     {board_papers_url}\")\n        print(\"  2. Right-click the PDF and copy its URL\")\n        print(\"  3. Paste into MANUAL_BOARD_PAPERS_URL in Cell 2\")\n        print(\"  4. Re-run Cell 2, then skip to Cell 5\")\n        print(\"=\"*60)\n        raise SystemExit(\"Set MANUAL_BOARD_PAPERS_URL to the direct PDF URL and re-run.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Download PDF\n\nif not pdf_paths:\n    save_dir = tempfile.mkdtemp(prefix=\"nhspapers_\")\n    print(f\"Downloading: {selected_url}\")\n    data = download_file(session, selected_url, board_papers_url)\n\n    if data is not None:\n        pdf_paths = save_and_unpack(data, save_dir)\n    else:\n        print()\n        print(\"Sorry this site blocks automated downloads - if you like you can manually\")\n        print(\"upload a board paper PDF to the file panel on the left, and I will process it.\")\n        print()\n        print(\"Steps:\")\n        print(\"  1. Download the PDF from the trust website in your browser\")\n        print(\"  2. Click the folder icon in Colab's left panel\")\n        print(\"  3. Click the upload icon and select your PDF\")\n        print(\"  4. Set MANUAL_PDF_PATH = 'your_filename.pdf' in Cell 2\")\n        print(\"  5. Re-run Cell 2, then skip to Cell 6\")\n        raise SystemExit(\"Upload PDF manually and set MANUAL_PDF_PATH in Cell 2.\")\n\nif pdf_paths:\n    print(f\"\\nReady: {[os.path.basename(p) for p in pdf_paths]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Extract text from PDF(s)\n\nif not pdf_paths:\n    print(\"No PDFs to process. Run Cell 5 first.\")\nelse:\n    print(\"Extracting text...\")\n    extracted = extract_targeted_text(pdf_paths)\n    total_chars = sum(len(v) for v in extracted.values())\n    print(f\"\\nDone: {len(extracted)} section(s), {total_chars:,} characters\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Analyse with Claude\n\nif 'extracted' not in dir() or not extracted:\n    print(\"No extracted text. Run Cell 6 first.\")\nelif PROMPT_TEMPLATE is None:\n    print(\"ERROR: prompt_template.txt not found. Upload it via the files panel.\")\nelse:\n    client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n\n    parts, total_chars = [], 0\n    for section, text in extracted.items():\n        if not text.strip(): continue\n        header = f\"\\n\\n=== {section.upper().replace('_', ' ')} ===\\n\"\n        if total_chars + len(header) + len(text) > CHAR_LIMIT:\n            print(\"Character limit reached — truncating\")\n            break\n        parts.append(header + text)\n        total_chars += len(header) + len(text)\n\n    combined_text = \"\".join(parts)\n    print(f\"Sending {total_chars:,} characters to {MODEL}...\")\n\n    prompt = (\n        PROMPT_TEMPLATE\n        .replace(\"{{TRUST_NAME}}\", TRUST_NAME)\n        .replace(\"{{BOARD_PAPERS_URL}}\", board_papers_url)\n        .replace(\"{{EXTRACTED_TEXT}}\", combined_text)\n    )\n\n    message = client.messages.create(\n        model=MODEL,\n        max_tokens=4096,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n    )\n\n    usage = message.usage\n    print(f\"Tokens: {usage.input_tokens:,} in / {usage.output_tokens:,} out\")\n    story_leads = message.content[0].text\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"STORY LEADS\")\n    print(\"=\" * 60 + \"\\n\")\n    print(story_leads)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Save and download results\n\nif 'story_leads' not in dir():\n    print(\"No results yet. Run Cell 7 first.\")\nelse:\n    safe_name = TRUST_NAME.replace(\" \", \"_\").replace(\"/\", \"-\")\n    output_file = f\"{safe_name}_leads.md\"\n\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        f.write(f\"# Story leads: {TRUST_NAME}\\n\\n\")\n        f.write(f\"Source: {board_papers_url}\\n\\n---\\n\\n\")\n        f.write(story_leads)\n\n    print(f\"Saved: {output_file}\")\n\n    try:\n        from google.colab import files\n        files.download(output_file)\n        print(\"Downloading to your computer...\")\n    except ImportError:\n        print(\"(Running locally — file saved to current directory)\")"
  }
 ]
}