{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "provenance": [],
   "name": "NHS Board Papers Analyser.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# NHS Board Papers Analyser\n\nEnter a trust name, run all cells in order, and receive structured story leads.\n\n**You will need:** An [Anthropic API key](https://console.anthropic.com) (create a free account, then go to API Keys).\n\n**Cost per run:** approximately £0.50–£1.50 depending on pack size (using Claude Opus). Use `claude-sonnet-4-6` in the config cell for ~5x cheaper results.\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: Install dependencies (run once per session)\n!pip install -q anthropic pypdfium2 duckduckgo-search requests beautifulsoup4\nprint(\"Dependencies installed.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Configure here\n\nEdit the three values below, then run the cell."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Configuration — edit these values\n\nANTHROPIC_API_KEY = \"sk-ant-...\"          # Your Anthropic API key\nTRUST_NAME        = \"Sussex Community NHS Foundation Trust\"\nMODEL             = \"claude-opus-4-6\"     # or \"claude-sonnet-4-6\" for cheaper runs\n\n# Optional: paste the board papers page URL here to skip the search step\n# Leave as \"\" to search automatically\nMANUAL_BOARD_PAPERS_URL = \"\"\n\n# Optional: if the download fails and you upload a PDF manually,\n# set this to the filename after uploading (e.g. \"board_papers.pdf\")\nMANUAL_PDF_PATH = \"\"\n\n# ── Validation ──\nif not ANTHROPIC_API_KEY.startswith(\"sk-\"):\n    print(\"WARNING: API key does not look right — check it starts with sk-ant-\")\nelse:\n    print(f\"Trust:     {TRUST_NAME}\")\n    print(f\"Model:     {MODEL}\")\n    print(f\"API key:   set ({ANTHROPIC_API_KEY[:12]}...)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Imports and helper functions\n\nimport os, io, re, sys, zipfile, tempfile\nfrom pathlib import Path\nfrom urllib.parse import urljoin\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pypdfium2 as pdfium\nimport anthropic\nfrom duckduckgo_search import DDGS\n\nFALLBACK_UAS = [\n    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n    \"(KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 \"\n    \"(KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n]\n\nBOARD_PAPER_KWS = [\n    \"board-papers\", \"board-meeting\", \"board-meetings\", \"trust-board\",\n    \"board-of-directors\", \"board_papers\", \"board-pack\",\n]\n\nCHARS_PER_PAGE = 3000\nCHAR_LIMIT     = 400_000\n\n# ── Search ──\ndef find_board_papers_url(trust_name):\n    queries = [\n        f'\"{trust_name}\" board papers 2025 OR 2026 site:nhs.uk',\n        f'\"{trust_name}\" board meeting papers site:nhs.uk',\n        f'\"{trust_name}\" NHS board papers minutes 2026',\n    ]\n    with DDGS() as ddg:\n        for query in queries:\n            try:\n                for r in ddg.text(query, max_results=8):\n                    url = r.get(\"href\", \"\")\n                    if any(kw in url.lower() for kw in BOARD_PAPER_KWS):\n                        return url\n            except Exception as e:\n                print(f\"  Search attempt failed: {e}\")\n    return None\n\n# ── Fetch index and find links ──\ndef get_document_links(session, index_url):\n    try:\n        resp = session.get(index_url, timeout=30)\n        resp.raise_for_status()\n    except Exception as e:\n        print(f\"  Could not fetch index page: {e}\")\n        return []\n    soup = BeautifulSoup(resp.text, \"html.parser\")\n    links, seen = [], set()\n    doc_exts = (\".pdf\", \".zip\", \".docx\")\n    doc_kws  = [\"download\", \"document\", \"/file\", \"attachment\", \"board-paper\"]\n    for a in soup.find_all(\"a\", href=True):\n        href = a[\"href\"]\n        text = a.get_text(strip=True) or href\n        h = href.lower()\n        if any(h.endswith(e) for e in doc_exts) or any(k in h for k in doc_kws):\n            full = href if href.startswith(\"http\") else urljoin(index_url, href)\n            if full not in seen:\n                seen.add(full)\n                links.append({\"text\": text[:100], \"url\": full})\n    return links\n\ndef pick_best_link(links):\n    if not links: return None\n    priority = [\"2026\",\"2025\",\"january\",\"february\",\"march\",\"november\",\n                \"board-pack\",\"combined\",\"agenda\"]\n    for link in links:\n        if any(t in (link[\"text\"]+\" \"+link[\"url\"]).lower() for t in priority):\n            return link[\"url\"]\n    for link in links:\n        if \".pdf\" in link[\"url\"].lower(): return link[\"url\"]\n    return links[0][\"url\"]\n\n# ── Download ──\ndef download_file(session, url, referer):\n    for i, ua in enumerate(FALLBACK_UAS):\n        headers = {\"User-Agent\": ua, \"Referer\": referer,\n                   \"Accept\": \"application/pdf,application/zip,*/*\"}\n        try:\n            resp = session.get(url, headers=headers, timeout=120)\n            if resp.status_code == 200 and len(resp.content) > 10_000:\n                return resp.content\n            print(f\"  Attempt {i+1}: HTTP {resp.status_code}\")\n        except Exception as e:\n            print(f\"  Attempt {i+1} failed: {e}\")\n    return None\n\ndef save_and_unpack(data, save_dir):\n    os.makedirs(save_dir, exist_ok=True)\n    if data[:2] == b\"PK\":\n        print(\"  ZIP detected — extracting PDFs...\")\n        paths = []\n        try:\n            with zipfile.ZipFile(io.BytesIO(data)) as zf:\n                for name in zf.namelist():\n                    if name.lower().endswith(\".pdf\") and not name.startswith(\"__MACOSX\"):\n                        safe = os.path.basename(name) or f\"file_{len(paths)}.pdf\"\n                        out = os.path.join(save_dir, safe)\n                        with open(out, \"wb\") as f: f.write(zf.read(name))\n                        paths.append(out)\n                        print(f\"  Extracted: {safe}\")\n        except zipfile.BadZipFile:\n            print(\"  ZIP extraction failed.\")\n        return paths\n    else:\n        out = os.path.join(save_dir, \"board_papers.pdf\")\n        with open(out, \"wb\") as f: f.write(data)\n        print(f\"  Saved: board_papers.pdf ({len(data):,} bytes)\")\n        return [out]\n\n# ── Extract text ──\ndef extract_pages(pdf, start, end):\n    parts = []\n    for i in range(start, min(end, len(pdf))):\n        try:\n            text = pdf[i].get_textpage().get_text_range()\n            if text.strip():\n                parts.append(f\"-- Page {i+1} --\\n{text[:CHARS_PER_PAGE]}\")\n        except Exception: pass\n    return \"\\n\".join(parts)\n\ndef find_section_starts(agenda_text, total):\n    patterns = {\n        \"ceo_report\":  r\"chief executive[^\\n]{0,60}?(\\d{1,3})\\b\",\n        \"finance\":     r\"finance report[^\\n]{0,60}?(\\d{1,3})\\b\",\n        \"performance\": r\"(?:integrated performance|ipr)[^\\n]{0,60}?(\\d{1,3})\\b\",\n        \"quality\":     r\"quality[^\\n]{0,60}?(\\d{1,3})\\b\",\n        \"workforce\":   r\"(?:people committee|workforce)[^\\n]{0,60}?(\\d{1,3})\\b\",\n    }\n    secs = {}\n    for name, pat in patterns.items():\n        m = re.search(pat, agenda_text.lower())\n        if m:\n            p = int(m.group(1))\n            if 3 <= p <= total: secs[name] = p - 1\n    return secs\n\ndef extract_targeted_text(pdf_paths):\n    all_secs = {}\n    for pdf_path in pdf_paths:\n        label = os.path.basename(pdf_path)\n        print(f\"  Reading: {label}\")\n        try: pdf = pdfium.PdfDocument(pdf_path)\n        except Exception as e:\n            print(f\"  Could not open: {e}\"); continue\n        total = len(pdf)\n        print(f\"  Pages: {total}\")\n        agenda = extract_pages(pdf, 0, min(6, total))\n        all_secs[f\"{label}__agenda\"] = agenda\n        secs = find_section_starts(agenda, total)\n        if secs:\n            print(f\"  Sections: {list(secs.keys())}\")\n            for sname, start in secs.items():\n                all_secs[f\"{label}__{sname}\"] = extract_pages(pdf, start, min(start+30, total))\n        else:\n            print(\"  No agenda refs found — reading in thirds\")\n            chunk = max(20, total // 3)\n            all_secs[f\"{label}__part_1\"] = extract_pages(pdf, 0, chunk)\n            all_secs[f\"{label}__part_2\"] = extract_pages(pdf, chunk, chunk*2)\n            all_secs[f\"{label}__part_3\"] = extract_pages(pdf, chunk*2, total)\n    return all_secs\n\n# ── Load prompt ──\nPROMPT_TEMPLATE = open(\n    Path(\"prompt_template.txt\") if Path(\"prompt_template.txt\").exists()\n    else Path(\"/content/prompt_template.txt\"), encoding=\"utf-8\"\n).read() if (Path(\"prompt_template.txt\").exists() or Path(\"/content/prompt_template.txt\").exists()) else None\n\nprint(\"Helper functions loaded.\")\nif PROMPT_TEMPLATE:\n    print(\"Prompt template loaded.\")\nelse:\n    print(\"WARNING: prompt_template.txt not found. Upload it to the files panel or the /content directory.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Find board papers page and document links\n\nif MANUAL_PDF_PATH:\n    print(f\"Using manually provided PDF: {MANUAL_PDF_PATH}\")\n    board_papers_url = MANUAL_BOARD_PAPERS_URL or \"(manual upload)\"\n    selected_url = MANUAL_PDF_PATH\n    pdf_paths = [MANUAL_PDF_PATH]\nelse:\n    board_papers_url = MANUAL_BOARD_PAPERS_URL\n\n    if not board_papers_url:\n        print(f\"Searching for board papers page: {TRUST_NAME}...\")\n        board_papers_url = find_board_papers_url(TRUST_NAME)\n        if board_papers_url:\n            print(f\"Found: {board_papers_url}\")\n        else:\n            print(\"Could not find automatically.\")\n            board_papers_url = input(\"Paste the board papers URL: \").strip()\n    else:\n        print(f\"Using provided URL: {board_papers_url}\")\n\n    session = requests.Session()\n    session.headers[\"User-Agent\"] = FALLBACK_UAS[0]\n    # Visit index page to pick up session cookies\n    try: session.get(board_papers_url, timeout=20)\n    except Exception: pass\n\n    print(\"\\nFetching document links...\")\n    links = get_document_links(session, board_papers_url)\n\n    if links:\n        print(f\"\\nFound {len(links)} document link(s):\")\n        for i, link in enumerate(links[:15]):\n            print(f\"  [{i}] {link['text'][:70]}\")\n        selected_url = pick_best_link(links)\n        print(f\"\\nAuto-selected: {selected_url}\")\n    else:\n        print(\"No links found on index page.\")\n        selected_url = input(\"Paste the direct PDF URL: \").strip()\n    pdf_paths = []"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Download PDF\n# If the download fails, the cell will tell you what to do.\n\nif not pdf_paths:  # skip if PDF already provided\n    save_dir = tempfile.mkdtemp(prefix=\"nhspapers_\")\n    print(f\"Downloading: {selected_url}\")\n    data = download_file(session, selected_url, board_papers_url)\n\n    if data is not None:\n        pdf_paths = save_and_unpack(data, save_dir)\n    else:\n        print(\"\"\"\nSorry this site blocks automated downloads - if you like you can manually\nupload a board paper PDF to the file panel on the left, and I will process it.\n\"\"\")\n        # Colab file upload\n        try:\n            from google.colab import files\n            print(\"Use the upload button above, or run: uploaded = files.upload()\")\n            uploaded = files.upload()\n            if uploaded:\n                fname = list(uploaded.keys())[0]\n                pdf_paths = [fname]\n                print(f\"Using uploaded file: {fname}\")\n        except ImportError:\n            # Running locally, not in Colab\n            path = input(\"Enter path to manually downloaded PDF: \").strip()\n            if path: pdf_paths = [path]\n\nif pdf_paths:\n    print(f\"\\nReady to process {len(pdf_paths)} PDF(s): {[os.path.basename(p) for p in pdf_paths]}\")\nelse:\n    print(\"No PDF to process. Check the download or upload a file manually.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Extract text from PDF(s)\n\nif pdf_paths:\n    print(\"Extracting text...\")\n    extracted = extract_targeted_text(pdf_paths)\n    total_chars = sum(len(v) for v in extracted.values())\n    print(f\"\\nExtracted {len(extracted)} section(s), {total_chars:,} characters total\")\nelse:\n    print(\"No PDFs to extract from. Run Cell 5 first.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Analyse with Claude\n\nif not pdf_paths or not extracted:\n    print(\"No extracted text. Run Cell 6 first.\")\nelif PROMPT_TEMPLATE is None:\n    print(\"ERROR: prompt_template.txt not found. Upload it to the files panel.\")\nelse:\n    client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n\n    # Build combined text up to character limit\n    parts = []\n    total_chars = 0\n    for section, text in extracted.items():\n        if not text.strip(): continue\n        header = f\"\\n\\n=== {section.upper().replace('_', ' ')} ===\\n\"\n        if total_chars + len(header) + len(text) > CHAR_LIMIT:\n            print(f\"  Character limit reached — truncating\")\n            break\n        parts.append(header + text)\n        total_chars += len(header) + len(text)\n\n    combined_text = \"\".join(parts)\n    print(f\"Sending {total_chars:,} characters to {MODEL}...\")\n\n    prompt = (\n        PROMPT_TEMPLATE\n        .replace(\"{{TRUST_NAME}}\", TRUST_NAME)\n        .replace(\"{{BOARD_PAPERS_URL}}\", board_papers_url)\n        .replace(\"{{EXTRACTED_TEXT}}\", combined_text)\n    )\n\n    message = client.messages.create(\n        model=MODEL,\n        max_tokens=4096,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n    )\n\n    usage = message.usage\n    print(f\"Tokens used: {usage.input_tokens:,} in / {usage.output_tokens:,} out\")\n    story_leads = message.content[0].text\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"STORY LEADS\")\n    print(\"=\" * 60 + \"\\n\")\n    print(story_leads)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Save output\n# Saves the story leads as a markdown file and (in Colab) downloads it.\n\nif 'story_leads' in dir():\n    safe_name = TRUST_NAME.replace(\" \", \"_\").replace(\"/\", \"-\")\n    output_file = f\"{safe_name}_leads.md\"\n\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        f.write(f\"# Story leads: {TRUST_NAME}\\n\\n\")\n        f.write(f\"Source: {board_papers_url}\\n\\n---\\n\\n\")\n        f.write(story_leads)\n\n    print(f\"Saved: {output_file}\")\n\n    # Auto-download in Colab\n    try:\n        from google.colab import files\n        files.download(output_file)\n        print(\"Downloading to your computer...\")\n    except ImportError:\n        print(f\"(Running locally — file saved to current directory)\")\nelse:\n    print(\"No story leads to save. Run Cell 7 first.\")"
  }
 ]
}
